{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "afceb3b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json \n",
        "import pickle\n",
        "import os \n",
        "import pandas as pd \n",
        "import json\n",
        "import time\n",
        "import math \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bf2a51e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPORTANT: ne jamais versionner de clés API.\n",
        "# CHANGE: clés API en dur -> variables d'environnement (évite de commit des secrets).\n",
        "# Les clés sont lues depuis des variables d'environnement (fichier .env ignoré par Git).\n",
        "\n",
        "headers_myapp = {\n",
        "    'API-Key': os.getenv('AFKLM_API_KEY_MYAPP', '<REDACTED>'),\n",
        "    'Accept': 'application/hal+json'\n",
        "}\n",
        "headers_myapp2 = {\n",
        "    'API-Key': os.getenv('AFKLM_API_KEY_MYAPP2', '<REDACTED>'),\n",
        "    'Accept': 'application/hal+json'\n",
        "}\n",
        "headers_extract1 = {\n",
        "    'API-Key': os.getenv('AFKLM_API_KEY_EXTRACT1', '<REDACTED>'),\n",
        "    'Accept': 'application/hal+json'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fdacd29",
      "metadata": {},
      "source": [
        "Set up extraction follow up file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce8ab59d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Appels pour avoir le nombre de pages par jour sur le mois de janvier\n",
        "import datetime\n",
        "startrange_ = datetime.datetime(2026, 1, 1)\n",
        "numdays = 32\n",
        "date_list = [startrange_ + datetime.timedelta(days=x) for x in range(numdays)]\n",
        "date_list = [sub.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\") for sub in date_list]\n",
        "\n",
        "totalPagesPerDay_ = []\n",
        "for i in range(len(date_list)-1):\n",
        "    #if i not in [0,1]:\n",
        "        try: \n",
        "            print(date_list[i])\n",
        "            print(date_list[i+1])\n",
        "\n",
        "            startrange_ = date_list[i]\n",
        "            endrange_ = date_list[i+1]\n",
        "            #endrange_ = \"2026-01-01T08%3A00%3A00.000Z\"\n",
        "\n",
        "            # Premier appel pour avoir le nombre de pages \n",
        "            response = requests.get(url=f\"https://api.airfranceklm.com/opendata/flightstatus?startRange={startrange_}&endRange={endrange_}\", headers=headers)\n",
        "            print(response.status_code)\n",
        "            totalPages = json.loads(response.content.decode('utf-8'))[\"page\"]['totalPages'] # Nombre total de pages \n",
        "            totalPagesPerDay_.append(totalPages)\n",
        "            time.sleep(5)\n",
        "        except:\n",
        "            print(f\"Call {startrange_} to {endrange_} failed with error {response.status_code}\")\n",
        "            totalPagesPerDay_.append(\"failed\")\n",
        "            time.sleep(5)\n",
        "\n",
        "# Create DF\n",
        "totalPagesPerDay_df = pd.DataFrame({\"from\" : date_list[0:len(date_list)-1],\n",
        "                                    \"to\" : date_list[1:len(date_list)],\n",
        "                                    \"totalPages\" : totalPagesPerDay_}\n",
        "                                    )\n",
        "\n",
        "# Add necessary columns for extraction \n",
        "totalPagesPerDay_df['lastExtractedPage'] = np.nan\n",
        "worker_array = [\"myapp\",\"myapp2\",\"extract1\"]\n",
        "totalPagesPerDay_df['worker'] = np.repeat(worker_array, math.ceil(totalPagesPerDay_df.shape[0]/3))[0:totalPagesPerDay_df.shape[0]]\n",
        "totalPagesPerDay_df['complete'] = np.repeat(False, totalPagesPerDay_df.shape[0])\n",
        "\n",
        "# Save DF\n",
        "totalPagesPerDay_df.to_csv(\"/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/api/AFKLM_totalPagesPerDay_df_jan26.csv\")\n",
        "sum(totalPagesPerDay_df.loc[totalPagesPerDay_df.totalPages != \"failed\"][\"totalPages\"].apply(lambda x:int(x))) * 100  # taille espérée de la base de données \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a77d3052",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>from</th>\n",
              "      <th>to</th>\n",
              "      <th>totalPages</th>\n",
              "      <th>lastExtractedPage</th>\n",
              "      <th>worker</th>\n",
              "      <th>complete</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2026-01-01T00:00:00.000000Z</td>\n",
              "      <td>2026-01-02T00:00:00.000000Z</td>\n",
              "      <td>87</td>\n",
              "      <td>87.0</td>\n",
              "      <td>myapp</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2026-01-02T00:00:00.000000Z</td>\n",
              "      <td>2026-01-03T00:00:00.000000Z</td>\n",
              "      <td>94</td>\n",
              "      <td>94.0</td>\n",
              "      <td>myapp</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2026-01-03T00:00:00.000000Z</td>\n",
              "      <td>2026-01-04T00:00:00.000000Z</td>\n",
              "      <td>failed</td>\n",
              "      <td>NaN</td>\n",
              "      <td>myapp</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2026-01-04T00:00:00.000000Z</td>\n",
              "      <td>2026-01-05T00:00:00.000000Z</td>\n",
              "      <td>failed</td>\n",
              "      <td>NaN</td>\n",
              "      <td>myapp</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2026-01-05T00:00:00.000000Z</td>\n",
              "      <td>2026-01-06T00:00:00.000000Z</td>\n",
              "      <td>failed</td>\n",
              "      <td>NaN</td>\n",
              "      <td>myapp</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          from                           to totalPages  \\\n",
              "0  2026-01-01T00:00:00.000000Z  2026-01-02T00:00:00.000000Z         87   \n",
              "1  2026-01-02T00:00:00.000000Z  2026-01-03T00:00:00.000000Z         94   \n",
              "2  2026-01-03T00:00:00.000000Z  2026-01-04T00:00:00.000000Z     failed   \n",
              "3  2026-01-04T00:00:00.000000Z  2026-01-05T00:00:00.000000Z     failed   \n",
              "4  2026-01-05T00:00:00.000000Z  2026-01-06T00:00:00.000000Z     failed   \n",
              "\n",
              "   lastExtractedPage worker  complete  \n",
              "0               87.0  myapp      True  \n",
              "1               94.0  myapp      True  \n",
              "2                NaN  myapp     False  \n",
              "3                NaN  myapp     False  \n",
              "4                NaN  myapp     False  "
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "totalPagesPerDay_df = pd.read_csv(\"/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/api/AFKLM_totalPagesPerDay_df_jan26.csv\", index_col=0)\n",
        "totalPagesPerDay_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7783ba5a",
      "metadata": {},
      "source": [
        "Set up extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3b545d68",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "folder exists\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    os.mkdir('/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/data/afklm_large/')\n",
        "except:\n",
        "    print(\"folder exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8178d1d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "myapp\n",
            "2026-01-07T00:00:00.000000Z\n",
            "Call page 40 failed with error 504\n",
            "2026-01-07T00:00:00.000000Z extrait.\n",
            "2026-01-08T00:00:00.000000Z\n",
            "Call page 29 failed with error 403\n",
            "2026-01-09T00:00:00.000000Z\n",
            "Call page 0 failed with error 403\n",
            "2026-01-10T00:00:00.000000Z\n",
            "Call page 0 failed with error 403\n",
            "2026-01-11T00:00:00.000000Z\n",
            "Call page 0 failed with error 403\n",
            "myapp2\n",
            "2026-01-14T00:00:00.000000Z\n",
            "Call page 72 failed with error 500\n",
            "Call page 88 failed with error 500\n",
            "2026-01-14T00:00:00.000000Z extrait.\n",
            "2026-01-15T00:00:00.000000Z\n",
            "Call page 1 failed with error 500\n",
            "Call page 6 failed with error 403\n",
            "2026-01-17T00:00:00.000000Z\n",
            "Call page 0 failed with error 403\n",
            "2026-01-18T00:00:00.000000Z\n",
            "Call page 0 failed with error 403\n",
            "2026-01-19T00:00:00.000000Z\n",
            "Call page 0 failed with error 403\n",
            "2026-01-21T00:00:00.000000Z\n",
            "Call page 0 failed with error 403\n",
            "2026-01-22T00:00:00.000000Z\n",
            "Call page 0 failed with error 403\n",
            "extract1\n",
            "2026-01-29T00:00:00.000000Z\n",
            "Call page 55 failed with error 500\n",
            "2026-01-29T00:00:00.000000Z extrait.\n",
            "2026-01-30T00:00:00.000000Z\n",
            "Call page 1 failed with error 500\n",
            "Call page 25 failed with error 403\n",
            "2026-01-31T00:00:00.000000Z\n",
            "Call page 0 failed with error 403\n"
          ]
        }
      ],
      "source": [
        "headers_attribution = {\"myapp\":headers_myapp,\n",
        "                       \"myapp2\":headers_myapp2,\n",
        "                       \"extract1\":headers_extract1}\n",
        "\n",
        "\n",
        "for worker_ in set(totalPagesPerDay_df['worker']):\n",
        "    print(worker_)\n",
        "    # Remaining work \n",
        "    sub_df = totalPagesPerDay_df.loc[(totalPagesPerDay_df['worker'] == worker_) & (-totalPagesPerDay_df['complete']) & (totalPagesPerDay_df['totalPages']!='failed')]\n",
        "    #  \n",
        "    for row_ in range(sub_df.shape[0]):\n",
        "        from_, to_, tp_, lep_ = sub_df.iloc[row_][[\"from\", \"to\",\"totalPages\", \"lastExtractedPage\"]]\n",
        "            \n",
        "        print(f\"{from_} commence.\")\n",
        "\n",
        "        if math.isnan(lep_):\n",
        "            start_page_ = 0\n",
        "        else:\n",
        "            start_page_ = int(lep_) + 1 \n",
        "        # Appel\n",
        "        for page_ in range(start_page_, int(tp_)+1):\n",
        "            try:\n",
        "                response = requests.get(url=f\"https://api.airfranceklm.com/opendata/flightstatus?startRange={from_}&endRange={to_}&pageNumber={page_}\", headers=headers_attribution[worker_])\n",
        "                with open(f'/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/data/afklm_large/AFKLM_get_flight_{from_[0:10].replace(\"-\",\"_\")}_{to_[0:10].replace(\"-\",\"_\")}_page_{page_}.json', 'w') as f:\n",
        "                    json.dump(response.json(), f, indent=4)\n",
        "                \n",
        "            except:\n",
        "                print(f\"Call page {page_} failed with error {response.status_code} --> Nombre de calls atteint pour ce worker.\")\n",
        "                if response.status_code == 403:\n",
        "                    totalPagesPerDay_df = pd.read_csv(\"/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/api/AFKLM_totalPagesPerDay_df_jan26.csv\", index_col=0)\n",
        "                    if start_page_ == page_:\n",
        "                        if start_page_ == 0 :\n",
        "                            lep_update = np.nan\n",
        "                        else:\n",
        "                            lep_update = page_ - 1\n",
        "                    else:\n",
        "                        lep_update = page_ - 1 \n",
        "                    totalPagesPerDay_df.loc[totalPagesPerDay_df[\"from\"] == from_,'lastExtractedPage']= lep_update\n",
        "                    totalPagesPerDay_df.to_csv(\"/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/api/AFKLM_totalPagesPerDay_df_jan26.csv\")\n",
        "                    break\n",
        "\n",
        "\n",
        "            if page_ == int(tp_):\n",
        "                totalPagesPerDay_df = pd.read_csv(\"/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/api/AFKLM_totalPagesPerDay_df_jan26.csv\", index_col=0)\n",
        "                totalPagesPerDay_df.loc[totalPagesPerDay_df[\"from\"] == from_,\"lastExtractedPage\"] = page_\n",
        "                totalPagesPerDay_df.loc[totalPagesPerDay_df[\"from\"] == from_,\"complete\"] = True\n",
        "                totalPagesPerDay_df.to_csv(\"/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/api/AFKLM_totalPagesPerDay_df_jan26.csv\")\n",
        "                print(f\"{from_} extrait.\")\n",
        "            time.sleep(5)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8dad84e9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-01-01T00:00:00.000000Z\n",
            "[0, 1, 3, 43]\n",
            "2026-01-02T00:00:00.000000Z\n",
            "[0, 1, 44, 73, 79, 80]\n",
            "2026-01-03T00:00:00.000000Z\n",
            "No data for this day\n",
            "2026-01-04T00:00:00.000000Z\n",
            "No data for this day\n",
            "2026-01-05T00:00:00.000000Z\n",
            "No data for this day\n",
            "2026-01-06T00:00:00.000000Z\n",
            "No data for this day\n",
            "2026-01-07T00:00:00.000000Z\n",
            "[18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]\n",
            "2026-01-08T00:00:00.000000Z\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]\n",
            "2026-01-09T00:00:00.000000Z\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92]\n",
            "2026-01-10T00:00:00.000000Z\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94]\n",
            "2026-01-11T00:00:00.000000Z\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]\n",
            "2026-01-12T00:00:00.000000Z\n",
            "[0]\n",
            "2026-01-13T00:00:00.000000Z\n",
            "[0, 1]\n",
            "2026-01-14T00:00:00.000000Z\n",
            "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]\n",
            "2026-01-15T00:00:00.000000Z\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "2026-01-16T00:00:00.000000Z\n",
            "No data for this day\n",
            "2026-01-17T00:00:00.000000Z\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94]\n",
            "2026-01-18T00:00:00.000000Z\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]\n",
            "2026-01-19T00:00:00.000000Z\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "2026-01-20T00:00:00.000000Z\n",
            "No data for this day\n",
            "2026-01-21T00:00:00.000000Z\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]\n",
            "2026-01-22T00:00:00.000000Z\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "2026-01-23T00:00:00.000000Z\n",
            "[0, 49]\n",
            "2026-01-24T00:00:00.000000Z\n",
            "No data for this day\n",
            "2026-01-25T00:00:00.000000Z\n",
            "No data for this day\n",
            "2026-01-26T00:00:00.000000Z\n",
            "No data for this day\n",
            "2026-01-27T00:00:00.000000Z\n",
            "No data for this day\n",
            "2026-01-28T00:00:00.000000Z\n",
            "[0, 1, 7, 43, 44, 45, 47, 51]\n",
            "2026-01-29T00:00:00.000000Z\n",
            "[0, 1, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92]\n",
            "2026-01-30T00:00:00.000000Z\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]\n",
            "2026-01-31T00:00:00.000000Z\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n"
          ]
        }
      ],
      "source": [
        "## Get missing pages\n",
        "\n",
        "dir_ = '/home/pierre/Documents/DE_DATASCIENTEST/data_projet_dst_airlines/afklm_large/'\n",
        "def get_missing_pages(monitoring_file, data_dir):\n",
        "\n",
        "    for f_ in monitoring_file[\"from\"]:\n",
        "        print(f_)\n",
        "        tp_ = monitoring_file.loc[monitoring_file['from']==f_,\"totalPages\"].iloc[0]\n",
        "        if (tp_ == 'failed'):\n",
        "            print(\"No data for this day\")\n",
        "        else:\n",
        "            p_range = list(range(0, int(tp_)+1))\n",
        "            files_ = os.listdir(data_dir)\n",
        "            files_ = list(filter(lambda x: x[17:27].replace(\"_\",\"-\") == f_[0:10], files_))\n",
        "            pages_ = [parse_file_name(x) for x in files_]\n",
        "            #print(p_range)\n",
        "            #print(pages_)\n",
        "            print(list(filter(lambda x: x not in pages_, p_range)))\n",
        "        \n",
        "\n",
        "def parse_file_name(file,):\n",
        "    from_ = file[17:27]\n",
        "    to_ = file[28:38]\n",
        "    page_ =  file.replace(f\"AFKLM_get_flight_{from_}_{to_}_page_\",\"\")\n",
        "    page_ =  page_.replace(\".json\",\"\")\n",
        "    return int(page_)\n",
        "\n",
        "\n",
        "get_missing_pages(totalPagesPerDay_df, dir_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97e1e728",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dts_airlines (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
