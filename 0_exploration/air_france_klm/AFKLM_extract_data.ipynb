{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afceb3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json \n",
    "import pickle\n",
    "import os \n",
    "import pandas as pd \n",
    "import json\n",
    "import time\n",
    "import math \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39131b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: ne jamais versionner de clés API.\n",
    "# CHANGE: clés API en dur -> variables d'environnement (évite de commit des secrets).\n",
    "# Les clés sont lues depuis des variables d'environnement (fichier .env ignoré par Git).\n",
    "\n",
    "headers_myapp = {\n",
    "    'API-Key': os.getenv('AFKLM_API_KEY_MYAPP', '<REDACTED>'),\n",
    "    'Accept': 'application/hal+json'\n",
    "}\n",
    "headers_myapp2 = {\n",
    "    'API-Key': os.getenv('AFKLM_API_KEY_MYAPP2', '<REDACTED>'),\n",
    "    'Accept': 'application/hal+json'\n",
    "}\n",
    "headers_extract1 = {\n",
    "    'API-Key': os.getenv('AFKLM_API_KEY_EXTRACT1', '<REDACTED>'),\n",
    "    'Accept': 'application/hal+json'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdacd29",
   "metadata": {},
   "source": [
    "Set up extraction follow up file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ab59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appels pour avoir le nombre de pages par jour sur le mois de janvier\n",
    "import datetime\n",
    "startrange_ = datetime.datetime(2026, 1, 1)\n",
    "numdays = 32\n",
    "date_list = [startrange_ + datetime.timedelta(days=x) for x in range(numdays)]\n",
    "date_list = [sub.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\") for sub in date_list]\n",
    "\n",
    "totalPagesPerDay_ = []\n",
    "for i in range(len(date_list)-1):\n",
    "    #if i not in [0,1]:\n",
    "        try: \n",
    "            print(date_list[i])\n",
    "            print(date_list[i+1])\n",
    "\n",
    "            startrange_ = date_list[i]\n",
    "            endrange_ = date_list[i+1]\n",
    "            #endrange_ = \"2026-01-01T08%3A00%3A00.000Z\"\n",
    "\n",
    "            # Premier appel pour avoir le nombre de pages \n",
    "            response = requests.get(url=f\"https://api.airfranceklm.com/opendata/flightstatus?startRange={startrange_}&endRange={endrange_}\", headers=headers)\n",
    "            print(response.status_code)\n",
    "            totalPages = json.loads(response.content.decode('utf-8'))[\"page\"]['totalPages'] # Nombre total de pages \n",
    "            totalPagesPerDay_.append(totalPages)\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            print(f\"Call {startrange_} to {endrange_} failed with error {response.status_code}\")\n",
    "            totalPagesPerDay_.append(\"failed\")\n",
    "            time.sleep(5)\n",
    "\n",
    "# Create DF\n",
    "totalPagesPerDay_df = pd.DataFrame({\"from\" : date_list[0:len(date_list)-1],\n",
    "                                    \"to\" : date_list[1:len(date_list)],\n",
    "                                    \"totalPages\" : totalPagesPerDay_}\n",
    "                                    )\n",
    "\n",
    "# Add necessary columns for extraction \n",
    "totalPagesPerDay_df['lastExtractedPage'] = np.nan\n",
    "worker_array = [\"myapp\",\"myapp2\",\"extract1\"]\n",
    "totalPagesPerDay_df['worker'] = np.repeat(worker_array, math.ceil(totalPagesPerDay_df.shape[0]/3))[0:totalPagesPerDay_df.shape[0]]\n",
    "totalPagesPerDay_df['complete'] = np.repeat(False, totalPagesPerDay_df.shape[0])\n",
    "\n",
    "# Save DF\n",
    "totalPagesPerDay_df.to_csv(\"/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/api/AFKLM_totalPagesPerDay_df_jan26.csv\")\n",
    "sum(totalPagesPerDay_df.loc[totalPagesPerDay_df.totalPages != \"failed\"][\"totalPages\"].apply(lambda x:int(x))) * 100  # taille espérée de la base de données \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d3052",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalPagesPerDay_df = pd.read_csv(\"/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/0_exploration/air_france_klm/AFKLM_totalPagesPerDay_df_jan26.csv\", index_col=0)\n",
    "totalPagesPerDay_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7783ba5a",
   "metadata": {},
   "source": [
    "Set up extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_attribution = {\"myapp\":headers_myapp,\n",
    "                       \"myapp2\":headers_myapp2,\n",
    "                       \"extract1\":headers_extract1}\n",
    "\n",
    "\n",
    "for worker_ in set(totalPagesPerDay_df['worker']):\n",
    "    print(worker_)\n",
    "    # Remaining work \n",
    "    sub_df = totalPagesPerDay_df.loc[(totalPagesPerDay_df['worker'] == worker_) & (-totalPagesPerDay_df['complete']) & (totalPagesPerDay_df['totalPages']!='failed')]\n",
    "    #  \n",
    "    for row_ in range(sub_df.shape[0]):\n",
    "        from_, to_, tp_, lep_ = sub_df.iloc[row_][[\"from\", \"to\",\"totalPages\", \"lastExtractedPage\"]]\n",
    "            \n",
    "        print(f\"{from_} commence.\")\n",
    "\n",
    "        if math.isnan(lep_):\n",
    "            start_page_ = 0\n",
    "        else:\n",
    "            start_page_ = int(lep_) + 1 \n",
    "        # Appel\n",
    "        for page_ in range(start_page_, int(tp_)+1):\n",
    "            try:\n",
    "                response = requests.get(url=f\"https://api.airfranceklm.com/opendata/flightstatus?startRange={from_}&endRange={to_}&pageNumber={page_}\", headers=headers_attribution[worker_])\n",
    "                with open(f'/home/pierre/Documents/DE_DATASCIENTEST/data_projet_dst_airlines/afklm_large/AFKLM_get_flight_{from_[0:10].replace(\"-\",\"_\")}_{to_[0:10].replace(\"-\",\"_\")}_page_{page_}.json', 'w') as f:\n",
    "                    json.dump(response.json(), f, indent=4)\n",
    "                \n",
    "            except:\n",
    "                print(f\"Call page {page_} failed with error {response.status_code}.\")\n",
    "                if response.status_code == 403:\n",
    "                    print(\"--> Nombre de calls atteint pour ce worker.\")\n",
    "                    totalPagesPerDay_df = pd.read_csv(\"/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/0_exploration/air_france_klm/AFKLM_totalPagesPerDay_df_jan26.csv\", index_col=0)\n",
    "                    if start_page_ == page_:\n",
    "                        if start_page_ == 0 :\n",
    "                            lep_update = np.nan\n",
    "                        else:\n",
    "                            lep_update = page_ - 1\n",
    "                    else:\n",
    "                        lep_update = page_ - 1 \n",
    "                    totalPagesPerDay_df.loc[totalPagesPerDay_df[\"from\"] == from_,'lastExtractedPage']= lep_update\n",
    "                    totalPagesPerDay_df.to_csv(\"/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/0_exploration/air_france_klm/AFKLM_totalPagesPerDay_df_jan26.csv\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            if page_ == int(tp_):\n",
    "                totalPagesPerDay_df = pd.read_csv(\"/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/0_exploration/air_france_klm/AFKLM_totalPagesPerDay_df_jan26.csv\", index_col=0)\n",
    "                totalPagesPerDay_df.loc[totalPagesPerDay_df[\"from\"] == from_,\"lastExtractedPage\"] = page_\n",
    "                totalPagesPerDay_df.loc[totalPagesPerDay_df[\"from\"] == from_,\"complete\"] = True\n",
    "                totalPagesPerDay_df.to_csv(\"/home/pierre/Documents/DE_DATASCIENTEST/dst_airlines/0_exploration/air_france_klm/AFKLM_totalPagesPerDay_df_jan26.csv\")\n",
    "                print(f\"{from_} extrait.\")\n",
    "            time.sleep(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76fa95",
   "metadata": {},
   "source": [
    "Get missing pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get missing pages\n",
    "\n",
    "dir_ = '/home/pierre/Documents/DE_DATASCIENTEST/data_projet_dst_airlines/afklm_large/'\n",
    "def get_missing_pages(monitoring_file, data_dir):\n",
    "    out_dict = {}\n",
    "    for f_ in monitoring_file[\"from\"]:\n",
    "        print(f_)\n",
    "        tp_ = monitoring_file.loc[monitoring_file['from']==f_,\"totalPages\"].iloc[0]\n",
    "        if (tp_ == 'failed'):\n",
    "            print(\"No data for this day\")\n",
    "        else:\n",
    "            p_range = list(range(0, int(tp_)+1))\n",
    "            files_ = os.listdir(data_dir)\n",
    "            files_ = list(filter(lambda x: x[17:27].replace(\"_\",\"-\") == f_[0:10], files_))\n",
    "            pages_ = [parse_file_name(x) for x in files_]\n",
    "            #print(p_range)\n",
    "            #print(pages_)\n",
    "            missing_pages_ = list(filter(lambda x: x not in pages_, p_range))\n",
    "            print(missing_pages_)\n",
    "            if len(missing_pages_)>0:\n",
    "                out_dict.update({f_: missing_pages_})\n",
    "    \n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def parse_file_name(file,):\n",
    "    from_ = file[17:27]\n",
    "    to_ = file[28:38]\n",
    "    page_ =  file.replace(f\"AFKLM_get_flight_{from_}_{to_}_page_\",\"\")\n",
    "    page_ =  page_.replace(\".json\",\"\")\n",
    "    return int(page_)\n",
    "\n",
    "\n",
    "missing_pages = get_missing_pages(totalPagesPerDay_df, dir_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c6354",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = [\"myapp\",\"myapp2\",\"extract1\"]\n",
    "\n",
    "worker_ = workers[0]\n",
    "\n",
    "print(worker_)\n",
    "for from_ in missing_pages.keys():\n",
    "    print(from_)\n",
    "    to_ = totalPagesPerDay_df.loc[totalPagesPerDay_df[\"from\"] == from_,\"to\"].iloc[0]\n",
    "    pages_ = missing_pages[from_]\n",
    "    for page_ in pages_:\n",
    "        print(page_)\n",
    "        try:\n",
    "            response = requests.get(url=f\"https://api.airfranceklm.com/opendata/flightstatus?startRange={from_}&endRange={to_}&pageNumber={page_}\", headers=headers_attribution[worker_])\n",
    "            with open(f'/home/pierre/Documents/DE_DATASCIENTEST/data_projet_dst_airlines/afklm_large/AFKLM_get_flight_{from_[0:10].replace(\"-\",\"_\")}_{to_[0:10].replace(\"-\",\"_\")}_page_{page_}.json', 'w') as f:\n",
    "                json.dump(response.json(), f, indent=4)\n",
    "            \n",
    "        except:\n",
    "            print(f\"Call page {page_} failed with error {response.status_code}.\")\n",
    "            if response.status_code == 403:\n",
    "                print(\"--> Nombre de calls atteint pour ce worker.\")\n",
    "                break\n",
    "        time.sleep(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pages = get_missing_pages(totalPagesPerDay_df, dir_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dts_airlines (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
